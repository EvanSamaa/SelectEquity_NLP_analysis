# -*- coding: utf-8 -*-
"""day_Jan_week_4_news_cluster.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wDkzRXgc7aPKNO66tThNOsMT9I3Urghn
"""
from utils import new_df_cleaning
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')

import re
NON_ALPHANUM = re.compile(r'[\W]')
NON_ASCII = re.compile(r'[^a-z0-1\s]')
def normalize_texts(texts):

  lower = texts.lower()
  no_punctuation = NON_ALPHANUM.sub(r' ', lower)
  no_non_ascii = NON_ASCII.sub(r'', no_punctuation)
  return no_non_ascii

def preprocess(news):
  news['raw_body']= news['raw_body'].astype(str)
  news['title'] = news['title'].astype(str)
  news['year'] = pd.DatetimeIndex(news['date']).year
  news['month'] = pd.DatetimeIndex(news['date']).month
  news['day'] = pd.DatetimeIndex(news['date']).day

  news['title']=news['title'].apply(normalize_texts)
  news['raw_body']=news['raw_body'].apply(normalize_texts)
  news['title']=news['title'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))
  news['raw_body']=news['raw_body'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))
  news['title'] = news['title'].apply(
    lambda x: ' '.join([word for word in x.split() if word not in (stopwords.words('english'))]))
  news['raw_body'] = news['raw_body'].apply(
    lambda x: ' '.join([word for word in x.split() if word not in (stopwords.words('english'))]))

  return news

def filter_scope(news,month,day_start,day_end):
  selected_news = news[(news['month']==month)]
  if day_start!=day_end:
      selected_news = selected_news[(day_start<=selected_news['day'])]
      selected_news = selected_news[(selected_news['day']<=day_end)]

      news_unit = {}
      for i in range(day_start,day_end+1):
        news_unit[i] = selected_news[selected_news['day']==i]
  else:
      news_unit = {}
      news_unit[0]=(selected_news[selected_news['day'] == day_start])
      selected_news=selected_news[selected_news['day'] == day_start]
  return news_unit,selected_news


def get_top_n_words(corpus, n=10):
  vec = CountVectorizer(stop_words='english').fit(corpus)
  bag_of_words = vec.transform(corpus)
  sum_words = bag_of_words.sum(axis=0)
  words_freq = [(word, sum_words[0, idx]) for word, idx in   vec.vocabulary_.items()]
  words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
  return words_freq[:n]

def trendy_words_per_news_unit(input):
  if isinstance(input, dict):
    news_unit=input
    for key in news_unit.keys():
      for field in ['title', 'raw_body']:
        words = []
        word_values = []
        for i,j in get_top_n_words(news_unit[key][field],15):
          words.append(i)
          word_values.append(j)
        fig, ax = plt.subplots(figsize=(16,8))
        ax.bar(range(len(words)), word_values);
        ax.set_xticks(range(len(words)));
        ax.set_xticklabels(words, rotation='vertical');
        ax.set_title('Top 15 words in the news %s day %d'%(field, key));
        ax.set_xlabel('Word');
        ax.set_ylabel('Number of occurences');
        plt.savefig('day%d_%s.jpg'%(key,field))
        plt.clf()
        plt.close()
  else:
      news=input
      for field in ['title', 'raw_body']:
        words = []
        word_values = []
        for i,j in get_top_n_words(news[field],15):
          words.append(i)
          word_values.append(j)
        fig, ax = plt.subplots(figsize=(16,8))
        ax.bar(range(len(words)), word_values);
        ax.set_xticks(range(len(words)));
        ax.set_xticklabels(words, rotation='vertical');
        ax.set_title('Top 15 words in the news %s during the period'%(field));
        ax.set_xlabel('Word');
        ax.set_ylabel('Number of occurences');
        plt.savefig('the_period_%s.jpg'%(field))
        plt.clf()
        plt.close()

def word_cloud_vis(news):
  from wordcloud import WordCloud
  all_words = ''.join([word for word in news['title'][0:100000]])
  wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)
  plt.figure(figsize=(15, 8))
  plt.imshow(wordcloud, interpolation="bilinear")
  plt.axis('off')
  plt.title("Some frequent words used in the headlines", weight='bold', fontsize=14)
  plt.show()

  all_words = ''.join(
    [word for word in news['raw_body'][0:100000000000000000000000000000000000000000000000000]])
  wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)
  plt.figure(figsize=(15, 8))
  plt.imshow(wordcloud, interpolation="bilinear")
  plt.axis('off')
  plt.title("Some frequent words used in the body", weight='bold', fontsize=14)
  plt.show()

#pretrained model word2vec
def load_pretrainedmodel():
  print('start loading')
  from gensim.models import KeyedVectors
  #pretrained_embeddings_path = "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
  word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
  return word2vec
  print('finish loading')

class df_sent_Vectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.dim = 300
    def fit(self, X, y):
        return self
    def transform(self, X):
        return np.array([
            np.mean([self.word2vec[w] for w in texts.split() if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for texts in X
        ])

def embed(news):
  wtv_vect = df_sent_Vectorizer(word2vec)
  title_train = pd.DataFrame(news['title'])
  body_train = pd.DataFrame(news['raw_body'])
  title_train_wtv = wtv_vect.transform(title_train.title)
  body_train_wtv = wtv_vect.transform( body_train.raw_body)
  return title_train,title_train_wtv,body_train,body_train_wtv

from sklearn.cluster import KMeans
n_clusters = 8
km = KMeans(n_clusters=n_clusters, init='random',n_init=10, max_iter=300,tol=1e-04, random_state=0)

def cluster(news):
  title_train, title_train_wtv, body_train, body_train_wtv = embed(news)
  title_km= km.fit_predict(title_train_wtv)
  df_title = pd.DataFrame({'title': title_train.title, 'topic_cluster': title_km})
  body_km = km.fit_predict(body_train_wtv)
  df_body = pd.DataFrame({'raw_body': body_train.raw_body, 'topic_cluster': body_km})
  return df_title,df_body

def top_words_per_cluster(df_title,df_body):
  for n in range(0, n_clusters):
    cluster_df_title = df_title[df_title['topic_cluster'] == n]

    words = []
    word_values = []
    for i, j in get_top_n_words(cluster_df_title['title'], 15):
      words.append(i)
      word_values.append(j)
    fig, ax = plt.subplots(figsize=(16, 8))
    ax.bar(range(len(words)), word_values)
    ax.set_xticks(range(len(words)))
    ax.set_xticklabels(words, rotation='vertical')
    ax.set_title('Top 15 words in the title cluster %d' % (n))
    ax.set_xlabel('Word')
    ax.set_ylabel('Number of occurences')
    #plt.show()
    plt.savefig('Top 15 words in the title cluster %d' % (n))
    plt.clf()
    plt.close()
  for n in range(0, n_clusters):
    cluster_df_body = df_body[df_body['topic_cluster'] == n]
    words = []
    word_values = []
    for i, j in get_top_n_words(cluster_df_body['raw_body'], 15):
      words.append(i)
      word_values.append(j)
    fig, ax = plt.subplots(figsize=(16, 8))
    ax.bar(range(len(words)), word_values)
    ax.set_xticks(range(len(words)))
    ax.set_xticklabels(words, rotation='vertical')
    ax.set_title('Top 15 words in the body cluster %d' % (n))
    ax.set_xlabel('Word')
    ax.set_ylabel('Number of occurences')
    #plt.show()
    plt.savefig('Top 15 words in the body cluster %d' % (n))
    plt.clf()
    plt.close()

if __name__ == '__main__':
  news = pd.read_csv('FT_all_between_1and2')
  #preprocess the news, input is raw news (pd),output is preprocessed news(pd)
  #news=preprocess(news)
  news=new_df_cleaning(news)
  news=preprocess(news)

  # get all the news for a specific time range,input preprocessed news(pd), month, day_start,day_end (int)
  #output:
  #news_unit is a dic containing news headline/body (pd) for each day
  #selected_news is a pd containing all the news in that time period
  news_unit,selected_news=filter_scope(news,1,1,31)

  #get the plots of trendy words for each news unit
  #input news_unit or selected_news
  #trendy_words_per_news_unit(news_unit)
  #trendy_words_per_news_unit(selected_news)

  #visualize the frequend word in wordcloud
  #input selected_news
  #word_cloud_vis(news_unit)

  #load pretrained embedding model
  word2vec=load_pretrainedmodel()

  #get the cluster of each news title/body
  #input selected_news (pd) or news_unit[i](pd)
  #output pd with news and corresponding cluster
  df_title,df_body=cluster(selected_news)

  #input title/body cluster (pd)
  #output the top words in each cluster (plot)
  top_words_per_cluster(df_title,df_body)




